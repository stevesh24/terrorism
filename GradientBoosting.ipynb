{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file 'GradientBoosting.ipynb' describes the Gradient Boosting analysis.\n",
    "outcome: gname, 63 categories, each having at least 150 entries\n",
    "features: 35, listed in final_data_list.xlsx\n",
    "rows (entries): 38251 total entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code gives the Gradient Boosting with the best settings found (n_estimators \n",
    "# = 200 and max_depth = 5). See the section below for a summary of the results.\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import sys\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "\n",
    "t1 = dt.datetime.now() \n",
    "\n",
    "home = r\"C:\\Users\\ibshi\\Desktop\\startup.ml\\challenge 2\\global terrorism\\data\"\n",
    "infile = home + r\"\\final_data.xlsx\"\n",
    "final_data = pd.read_excel(infile)\n",
    "\n",
    "infile = home + r\"\\Y_out.xlsx\"\n",
    "Y = pd.read_excel(infile)\n",
    "\n",
    "infile = home + r\"\\Y_in.csv\"\n",
    "Y1 = pd.read_csv(infile,names=['gname'])\n",
    "                 \n",
    "# Make the categorical variables into a series of dummy variables. \n",
    "\n",
    "catlist = ['country',\n",
    "'region',\n",
    "'attacktype1',\n",
    "'attacktype2',\n",
    "'targtype1',\n",
    "'targsubtype1',\n",
    "'claimmode',\n",
    "'weaptype1',\n",
    "'WeapRecode1',\n",
    "'weaptype2',\n",
    "'WeapRecode2',\n",
    "'hostkidoutcome']\n",
    "\n",
    "for cat in catlist:\n",
    "    hold = pd.get_dummies(final_data[cat])\n",
    "    final_data = pd.concat([final_data, hold], axis=1)\n",
    "\n",
    "final_data.drop(catlist,inplace=True,axis=1)\n",
    "\n",
    "#impute NaN values (replace with mean or median)\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "imputed_data = pd.DataFrame(imp.fit_transform(final_data))\n",
    "\n",
    "# Select entries randomly for test and training sets\n",
    "\n",
    "prop_train = 0.5\n",
    "numtrain = int(prop_train* len(Y1))\n",
    "\n",
    "sorter = pd.DataFrame(data=np.zeros(len(Y1)),columns=['sorter'])\n",
    "sorter.sorter[0:numtrain-1] = 1\n",
    "np.random.shuffle(sorter.sorter)\n",
    "\n",
    "t_data = pd.concat([sorter,imputed_data],axis=1)\n",
    "t_Y = pd.concat([sorter,Y1],axis=1)\n",
    "\n",
    "train = t_data[t_data.sorter == 1]\n",
    "trainY = t_Y[t_Y.sorter == 1]\n",
    "\n",
    "test = t_data[t_data.sorter == 0]\n",
    "testY = t_Y[t_Y.sorter == 0]\n",
    "\n",
    "train.drop(['sorter'],inplace=True,axis=1)\n",
    "trainY.drop(['sorter'],inplace=True,axis=1)\n",
    "test.drop(['sorter'],inplace=True,axis=1)\n",
    "testY.drop(['sorter'],inplace=True,axis=1)\n",
    "\n",
    "# put Y in proper structure for gradient boosting\n",
    "\n",
    "trainYa = np.ravel(trainY)\n",
    "testYa = np.ravel(testY)\n",
    "\n",
    "# run Gradient Boosting\n",
    "\n",
    "GB = sklearn.ensemble.GradientBoostingClassifier(n_estimators=200,\n",
    "max_depth=5)\n",
    "\n",
    "GB.fit(train,trainYa)\n",
    "out = GB.predict(test)\n",
    "outDF = pd.DataFrame(out)\n",
    "sc_test = GB.score(test,testYa)\n",
    "print \"accuracy on test set\"\n",
    "print sc_test\n",
    "sc_train = GB.score(train,trainYa)\n",
    "print \"accuracy on train set\"\n",
    "print sc_train\n",
    "\n",
    "t2 = dt.datetime.now()\n",
    "dt.timedelta.seconds\n",
    "tdiff = t2 - t1\n",
    "print tdiff.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Gradient Boosting are found in <results gradient boosting.xlsx>.\n",
    "The default settings gave test set accuracy = 0.913002562 and \n",
    "training set accuracy =\t0.968207488.\n",
    "\n",
    "Some tuning was attempted. Varying learning_rate from the default 0.1 to either\n",
    "0.05 or 0.2 did not improve the fits, and thus the default learning rate was\n",
    "used subsequently.  \n",
    "\n",
    "The number of estimators=200 gave somewhat better fits than 100 (default)and 400, \n",
    "and thus that value was used subsequently.\n",
    "\n",
    "Varying subsample, min_samples_split, andmin_samples_leaf did not seem to have large \n",
    "effects in the fits compared to the default values.\n",
    "\n",
    "Varying max depth did seem to have an effect (see 'GB max depth.jpg'). At max_depth = 5\n",
    "accuracy test and accuracy training seem to asymptote. Thus, these settings \n",
    "(number of estimators = 200 and max_depth = 5) were determined to give the best model.\n",
    "\n",
    "accuracy test = 0.91562\t\n",
    "accuracy training = 0.99885\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
